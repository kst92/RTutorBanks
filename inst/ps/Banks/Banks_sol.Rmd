# Problemset Effect of Banks

How important are banks for development - an Interactive Analysis with R

Author:  Katharina Stueckelmaier


#< ignore
```{r "create_ps"}
library(restorepoint)

# facilitates error detection
#set.restore.point.options(display.restore.point=TRUE)

library(RTutor)
#library(restorepoint)
setwd("C:/Users/Kathi Kaktus/Documents/Masterarbeit/Beispiel")
ps.name = "Banks"; sol.file = paste0(ps.name,"_sol.Rmd")
libs = c("foreign", "ggmap", "dplyr", "ggplot2", "lfe", "stargazer", "yaml", "maps", "plm") # character vector of all packages you load in the problem set
#name.rmd.chunks(sol.file) # set auto chunk names in this file
create.ps(sol.file=sol.file, ps.name=ps.name, user.name=NULL,libs=libs, stop.when.finished=FALSE, addons="quiz")


show.shiny.ps(ps.name, load.sav=FALSE,  sample.solution=!TRUE, is.solved=FALSE, catch.errors=TRUE, launch.browser=TRUE)
stop.without.error()
```
#>

## Welcome

Welcome to this interactive Problem Set, which is part of my master thesis at the University of Ulm.
The topic is about the analysis of "How important are banks for development", based on the paper "How important are banks for development? National banks in the united states, 1870-1900" written by Scott L. Fulford. It was published in December 2015 at "The Review of Economic and Statistics". 

The paper, a supplemental appendix and data are available at the following websites:

- **paper**: https://www.mitpressjournals.org/doi/pdf/10.1162/REST_a_00546
- **appendix**: https://www.mitpressjournals.org/doi/suppl/10.1162/REST_a_00546/suppl_file/REST_a_00546-esupp.pdf
- **data**: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PQ6ILM

## Exercise Content

1. Introduction

2. Historical Background

3. Reduced Form Regression and the Effect of Minimum Requirement

  3.1 Reduced Form Regression for Unconstrained Entry
  
  3.2 Capital Stock and Minimum Requirement: An Overview
  
  3.3 Effect of Constrained Banking on Production
  
  3.4. Solving the Endogeneity Problem

5. Estimation with Conditional Mean 

6. Robustness Check

7. Conclusion

8. References



#### Set-up of the interactive R-Tutorial:

In the following you can execute various exercises, to give you an understanding of our topic. Furthermore you will learn how to use R for solving statistical problems. I recommend to do the exercises step by step, but if you like to you can choose the order on your own, since every exercise can be solved independently.
During the exercises you can collect awards for solving tasks right. Additional you have the possibility to receive points, for answering quizzes right. 

Good luck!

## Exercise 1 - Introduction


Financial institute nowadays settle where they expect to make the most profits. How was the situation during economic growth times? How important were the connection and the accessibility to financial institutes during growth and could financial institutes open a bank where and when they liked to? *How important were banks during the time of development?* 

To open a bank some capital stock is needed. Furthermore, we can assume that a bank wouldn't open a branch at a special place without gaining profit out of this, since banks always try to reach profit-maximization. Under which conditions would a bank choose what place to enter? Places with more productivity are usual more attractive for banks. Let's check the relationship between the available capital stock and the production per person in a county.


**Task:** 

I give you the code for a scatter plot, where we display the relationship between capital stock and production per person. Our data for the plot contains the situation in the United States in the year 1880. Just press `edit` and `check`. 

```{r}
#< task_notest
plotData = read.table("Data2.tab", sep="\t", header=TRUE)
library(dplyr)
plotData = mutate(plotData, ln_tval_pc_100 = ln_tval_pc*100, capitalstockAV=capitalstock_pc*14755, year=1880)

#create the plot:
plot( plotData$capitalstockAV, plotData$ln_tval_pc_100, col="yellow", xlab="Capital Stock", ylab ="Production per person", xlim=(c(0,1500)), ylim =(c(0,1500)))
abline(lm(ln_tval_pc_100~capitalstockAV, data = plotData ), col="red")
#>
```

#< info "plot"
With the function `plot()` you can create several kind of plots, like box plots or scatter plots. The `abline()` function draws a straight line into a plot, for example a regression line like we did. 

For both functions there are several options to design the plot. `col` defines the color. With `xlab` and `ylab` it is possible to describe the axes and `xlib` and `ylib` specify the scale of the axes.
(Luhmann (2013), p.163-169)
#>

The yellow points display the observations. The red line shows the relationship between production per person and capital stock. As you can see in the plot, the production per person in a county is positive related with the capital stock of the counties bank. 
The more productive the county is, the more capital stock a bank need to provide. Under this assumption, we can try to answer our main question (*How does the existence of a bank effects the development?*) by regressing production per person based on the bank's capital stock. The regression equation then will have following form:

$$Y_{ct} = \gamma_{0} + \gamma_{1} C_{ct}/P_{ct} + \epsilon $$
Our outcome of interest is the production per person, displayed in the equation as $Y_{ct}$. As we saw in the plot, this variable is related to the capital stock of a county. Thus our dependent variable will be $C_{ct}/P_{ct}$, the capital stock at the time $t$ in a county $c$ divided by the average population at this time $t$ in county $c$. Our aim is to estimate the regression and to get the causal effect $\beta_{1}$. 

But as banks always try to maximize their profit they usually want to enter to places with high productivity. Assuming this, we will receive reverse causality in this regression model. 

#< info "Reverse Causality"
Reverse causality means that if we have changes in $Y$, this will also cause changes in $X$.
#>

In the following we will try to find out, how we can avoid reverse causality and achieve a causal effect.

Therefore, we will use a period of rapid financial, economic and geographic growth in the United States (Fulford (2015)) to do the analysis. The period will be from 1870 to 1900, right after the Civil War in the United States was finished. In the next exercise I will give you a short overview of the historical background during this time.


## Exercise 2 - Historical Background

> President Lincoln's address at Gettyburg. November 19, 1863:
"Four score and seven years ago our father brought forth on this continent a new nation conceived in liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war; testing whether that nation, or any nation so conceives, and so dedicated, can long endure. We are met on a great battlefield of that war. We have come to dedicated a portion of that field as a final resting-place for those who here gave their life that that nation might live." 
>

(Brady (1977), p.3)


**U.S. Civil war, 1861-1865**

During the period of 1861 to 1865, there was a civil war in the United States. The conflict was between the northern states, so called `unionstates`, and the southern states. The southern states separated themselves from the United States to create the `Confederate States of America (CSA)`. 

The reason were several political, social as well as economical disagreements. The main controversy was the slavery of black people. As 1860 Abraham Lincoln got voted for president, the southern states separated since they were pro-slavery. Consequently it came to a war. But as the CSA weren't accepted as a state from the rest of the world, they didn't get any support. Thus they hadn't really a chance against the industrial advanced union states. 1865 the Union finally won the war and slavery were abandoned. 

*Map of USA*

In the following map of the USA, the states of CSA are marked. Just press `check`.


```{r}
#< task_notest
require(maps)
library(maps)
CSA = c("alabama", "arkansas", "florida", "georgia", "louisiana", "mississippi", "north carolina", "south carolina", "tennessee", "texas", "virginia")
#namevec <- map(database = "state", col = "blue",fill=T, namesonly=TRUE)
map(database = "state", col = c("white", "blue")[1+(map(database = "state", col = "blue",fill=T, namesonly=TRUE) %in% tolower(CSA) )],fill=T )
#>
```



Since the fight was really expensive, the government of the United States created the `National Banking Act of 1863` to resolve the financial crises.


**National Banking Act of 1863**

The National Banking Act established a national banking system, to resolve the financial crisis during the civil war. The main aim was to assure federal control over the banking system. Furthermore, they wanted to create a uniform national currency, by introducing this national banking system. But what was the difference between the new national banks and the old available state banks?

State banks received their charter from the state government, whether national banks got them from the federal government. Thus, the federal government had control over national banks, but not over state banks. 
Furthermore national banks had higher capital requirements, than state banks. To enter in a certain state, they had to have a capital stock of minimum `50.000$`. Moreover they were restricted from making real loans or lending money over ten percent of the capital of the bank to anyone, to improve liquidity. Additionally, they were regulated by the office of Comptroller of the Currency (OCC), a treasury department created during this Banking Act, who also was responsible to print all national bank notes.

Because of the growth of the national banks, many state banks transformed to national banks. Therefore, already over 1600 national banks existed and around 300 state banks were left in 1870. National bank started to be the most important financial institute in the country. After the civil war finished, the United States had an immense financial, economical as well as geographical growth. Thus, national bank got even more important since they were the only ones who could issue money directly in form of bank notes. Though they still were constrained by the minimal size of capital stock. Some counties, where they maybe could have made big profits, couldn't afford this minimum size. On the other hand, some counties had even more capital, but opened their national banks with the minimum size of capital stock.

We resume:

1. National banks had to afford a minimum capital stock, we will call this "minimum size requirement" 

2. National banks weren't allowed to have any branches


What was the benefit of having a national bank for the residents? 

One goal of national banks is that they could provide liquidity and thus working capital. Furthermore, they created a uniform bank note currency, so that international trading became easier. To stabilize this system, national banks were not allowed to take land as collateral, what was problematic for farmers. Their main investment activities were providing liquidity and moving funds, as Scott L. Fulford claims in his paper.

In the next exercises we will look, how opening a national bank in certain counties affected the development and production there. 
Our dataset contains data about the geographical location and the size of national banks in 1870, 1880, 1890 and 1900. Since there weren't any branches, we can analyze the meaning of local financial development during this period.


**County Statistics**


Before starting to analyze we want to check some variables to get used to the data. Notice, that counties with an urban population of 50.000$ or more in 1880 are excluded in this table. Thus, counties with major cities are excluded, since these counties are not constrained by the minimum capital stock requirement. Note that average values are displayed in the table. (ct. Fulford (2015), p. 924)

<style>
.mytable {
  border-collapse: collapse;
  margin-bottom: 15px;
}
.mytable td {
  padding: 10px 15px;
}
.mytable th {
  padding: 10px 15px;
  border-bottom: 1px solid #ddd;
}
.right-border {
  border-right: 1px solid #ddd;
}
.bigger-font-size {
  font-size: 18px;
}
</style>
  
  <table class="mytable">
  <tr>
  <th class="right-border">Variable Type</th>
  <th>1870</th>
  <th>1880</th>
  <th>1890</th>
  <th>1900</th>
  </tr>
  <tr>
  <td class="right-border">County population</td>
  <td>11.660</td>
  <td>14.703</td>
  <td>17.955</td>
  <td>21.330</td>
  </tr>
  <tr>
  <td class="right-border">Counties</td>
  <td>2.665</td>
  <td>2.745</td>
  <td>2.743</td>
  <td>2.745</td>
  </tr>
  <tr>
  <td class="right-border">Number of banks</td>
  <td>0.41</td>
  <td>0.57 </td>
  <td>1.06</td>
  <td>1.09</td>
  </tr>
  <tr>
  <td class="right-border">Distance to closest bank (km)</td>
  <td>153.2</td>
  <td>89.4 </td>
  <td>42.5</td>
  <td>40.1</td>
  </tr>
  <tr>
  <td class="right-border">Capital stock per 1.000 capita</td>
  <td>1.96</td>
  <td>2.30</td>
  <td>4.46</td>
  <td>3.46</td>
  </tr> 
  <tr>
  <td class="right-border">Total production per capita</td>
  <td>104.0</td>
  <td>80.1</td>
  <td>96.9</td>
  <td>155.0</td>
  </tr>   
  <td class="right-border">Manufacturing value per capita</td>
  <td>38.5</td>
  <td>32.3</td>
  <td>44.7</td>
  <td>60.4</td>
  </tr>   
  <td class="right-border">Farm production value per capita</td>
  <td>65.5</td>
  <td>47.8</td>
  <td>51.0</td>
  <td>94.5</td>
  </tr>   
  </table>

First, we recognize that the average county population raised for about nearly 10.000 from 1870 to 1900. Since we look at the period right after the Civil War, who caused a lot of deaths, it makes sense that the population was smaller at the beginning of our period.

The capital stock per capita increased during this period. Since there was more capital, it was also possible to open more banks. That's why the number of banks increased rapidly during this period. Accordingly, the distance to the next banks decreased for many counties. 

If we take a deeper look at the manufacturing and farm production values, we recognize that first these values decreased and then increased again. Since time effects are included, this swap is explained by the significant deflation during this period, particularly around the Resumption Act of 1875, how Scott L. Fulford stated.



## Exercise 3. - Reduced Form Regression and the Effect of the Minimum Requirement

In this exercise we will try to estimate our equation of interest

$$Y_{ct} = \gamma_{0} + \gamma_{1} C_{ct}/P_{ct} + \epsilon $$
to receive a causal estimator. 

First, I will introduce a reduced form regression of this equation when banks are unconstrained. In this part we will ignore the minimum capital stock requirement.

Second, we will take care of the required minimum size and check how this affects the capital stock in the United States during the period of 1870 to 1900.

Afterwards, we will do again a reduced form regression including the requirement to find out how to estimate our regression model properly. 

Through the exercise, we will try to improve and expand our regression with the aim to reach causality. And at the end I will introduce a way how to refuse endogeneity in our model.


## Exercise 3.1. - Reduced Form Regression for Unconstrained Entry

In this exercise we will try to estimate our equation of interest 

$$Y_{ct} = \gamma_{0} + \gamma_{1} C_{ct}/P_{ct} + \epsilon $$

First, we have to load our data. The following info box explains how data can be loaded in R.

#< info "read.table"
With `read.table()` it is possible to read in the data `data.tab`.
```{r "1_1___Minimum_Capita",eval=FALSE}
read.table("data.tab", sep="\t", header=TRUE)
```
Furthermore, you can store the data via a variable-name like this:
```{r "1_1___Minimum_Capita__2",eval=FALSE}
MyVariable = read.table("data.tab", sep="\t", header=TRUE)
```
You have to use `header=TRUE` to make sure, that R uses the first line to get information for the header. With `sep="\t"` we tell R, that our data is separated by tab. Other possibilities could be for example `sep=","` or `sep=" "`.
To learn more about `read.table()` and how to read in different data sets follow the link:

"http://www.r-tutor.com/r-introduction/data-frame/data-import".
#>

To start with an exercise, you often must click first `edit` to work on the code. Enter the correct code and afterwards press `check`, to check your code. If you stuck at an exercise just click `hint` to get some help or `solution` to get the result.

**Task:** 

Read in your first data set `Data2.tab` and save it under `Dat1`. Replace `??` and `#`.

```{r "1_1___Minimum_Capita__3"}
#< task
# ?? = read.table("??", sep="\t", header=TRUE)
#> 
Dat1 = read.table("Data2.tab", sep = "\t", header = TRUE)
```

#< award "Read in a table"
Awesome! You solved your first exercise and received your first award. 
You know now how to read in tables. That's quite important to do an analysis with data.
Keep going on and try to collect more awards.
#>

Furthermore I will do some more settings for our dataset. Please press `check`.

```{r}
#< task_notest
library(dplyr)
# create some variables
Dat1 = mutate(Dat1, capitalstock50pc = capitalstock_pc*14755 / 50, insample = insample_allrural & tpop>1000, L.capitalstock = lag(capitalstock))

#filter after specific conditions
Dat1 = filter(Dat1, capitalstock<=500, !is.na(L.capitalstock), L.capitalstock <=500, capitalstock50pc <= 5,  capitalstock50pc>0, insample)
#>
```

Now our data is ready for the estimation. Please read through the following info boxes to get to know how to estimate linear models.

#< info "linear model"
Have a look at the simple equation 

$$Y = \alpha + \beta X + \epsilon$$. 

- $Y$ : dependent variable
- $X$ : explanatory variable
- $\alpha$, $\beta$ : regression coefficient
- $\epsilon$ : error term

Our regression equation would look like $\hat Y = \alpha + \beta X$, where $\hat Y$ shows the estimated value of $Y$. 

Let's describe this on a simple example. We have a sample of 50 people who we ask for their weight, which we save under `statement`. Afterwards we weight them and note the values under `scales`. All values are saved in the data set `weight`. Now we want to find out the relationship between `statement` and `scales`. To do this, we draw following simple linear regression model: $\hat {scales} = \alpha + \beta * scales$. Using R we can estimate this regression by `lm()` (Hatzinger, Hornik, Nagel (2011), pp.288).

```{r eval=FALSE}
# general form of lm()
lm(dependent variable ~ explantory variable, data= dataset)

# in our example
lm(scales ~ statement, data= weight)
```

Read more here: 

https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html
#>

#< info "Multiple Linear Regression"
In the info box above, you learned how linear models are compiled and how to solve them with R. Now, we want to use this information to calculate multiple linear models.

Multiple linear models have more than one explanatory variable. In general the equation we want to estimate looks like 

$$Y = \alpha + \beta_{1}X_{1} + ... + \beta_{n}X_{n} + \epsilon$$

- $Y$ : dependent variable
- $X_{1},...X_{n}$ : explanatory variables
- $\alhpa$, $\beta_{1},...,\beta_{n}$ : regression coefficients
- $\epsilon$ : normal distributed error term
- $n$ : number of data points

The regression equation is of the form:

$\hat Y = \alpha + \beta_{1}X_{1} + ... + \beta_{n}X_{n}$

Somehow, the values of $\alpha$ and $\beta_{1},...,\beta_{n}$ has to be estimated. This procedure is called `Ordinary Least Square Estimation`, short `OLS Estimation`.  

How can we make use of `lm()` in this case? We just need to bind all predictors by `+`.

```{r eval=FALSE}
lm(Y ~ X_{1} + ...+ X_{n}, data= dataset)
```
#>

We will put now time effects in our equation an try to estimate it this way:

$$Y_{ct} = \gamma_{t} + \gamma_{1} C_{ct}/P_{ct} + \epsilon $$


**Task:**

Regress log-production per person `ln_tval_pc` on `capitalstock50pc` and `factor(year)` of the data set `Dat1`. Make use of the function `lm()` and save it under the variable `lm1`.

```{r "2_1___Estimation_a_3"}
#< task
#lm1 = (??)
#>
lm1 = lm(ln_tval_pc~ capitalstock50pc + factor(year), data =Dat1)
```
#< award "Regression Level 1"
Great! You performed your first linear regression using `lm()`.  
#>

The function `summary()` is useful to display regression results.
Let's try it out on our first regression `lm1`. Press `check`.

```{r}
#< task_notest
summary(lm1)
#>
```

#< info "Output of summary()"
The function `summary()` gives a large output. The interesting part for us, can be found under `Coefficients`:

*Estimate:*  Shows us the value of the estimator $\hat \beta _{n}$ for each $\beta_{n}$. In our case it shows us how much more capital per capita we will get.

*Std.Error:* Standard error measure the average amount that the estimates differ from the actual average value of the explanatory variable.

*t value:* shows how much standard deviation the estimated coefficient $\beta _{n}$ is away from `0`. If the value is far away from `0`, we can reject the Null-Hypothesis. If this is the case, we can assume that there is a relationship between our dependent variable and the explanatory variables. 

*Pr(>|t|):* here we find the p-value for a T-Test.

#>

Try to answer following question:

#< quiz ("InterpretOutput")
question: How much more output per person will be reached by adding a bank to a county? Be aware that "capitalstock50pc" is in units of adding a bank to the county. Type in a decimal with three degrees after the point, e.g.. 1.123.

answer: 6.115
#>

Thus, if a national bank opens in a county the production per person can be `6.12%` higher compared to an unbanked county.
Remember our model equation. We said that we wanted to estimate the effect $\beta_{1}$. The aim of every regression is that this effect is a casual effect, meaning that the dependent variable depends on the explanatory variable (Field, Mile, Field (2012), p.14).

#< info "Correlation and Causality"

Correlation measures the intensity of the relationship between two variables. Changing one variable involves a particular change in the other one. The correlation coefficient between $Y$ and $X$ is defined as:

$$Corr(Y,X) = Cov(Y,X) /(sd(Y)*sd(X)).$$ 
- $Corr(Y,X)=0$, $Y$ and $X$ are independent
- $-1<=Corr(Y,X)<=1$, $Y$ and $X$ are correlated and called **uncorrelated random variables**.

With the function `cor(Y,X)` we can calculate the correlation coefficient in R.

Causality is given, if one variable determines the outcome of another variable. That means, changes in the second variable is a consequence of changes in the first one.

Causality implies a non-zero correlation. The other way doesn't hold.


(Field, Mile, Field (2012), Chapter 6 and Panik (2012), Chapter 12.B and Fahrmeier, Heumann, Kuenstler, Pigeot, Tutz (2016), Chapter 3.5)
#>

**Question**

#< quiz "Correlation"
question: Are ln_tval_pc and capitalstock50pc correlated?
sc:
  -yes*
  -no
succes: Correct! We have a positive correlation between the two variables.
failure: Try again.
#>

We have positive correlation. That means, getting higher values for `ln_tval_pc` will lead to higher values for `capitalstock50pc` in average (Hatzinger, Hornik, Nagel (2011), p.278).


The dependent variable `ln_tval_pc` and the explanatory variable `capitalstock50pc` from our model are correlated. But we can't assume that we estimated the causal effect.


Think back at the second exercise, where I explained the *National Currency Act of 1863*. One consequence was, that the national banks had to follow the **minimum capital stock requirement**. If a national bank wanted to enter at a county they had to have at least a capital stock of `50.000$`. Thus $C_{ct}$ is not only depending of the strategy of the bank, it now also depends on the minimum requirement. Furthermore, the capital stock size depends on how productive a county is. We claim here reverse causality.

#< info "Reverse Causality"
Reverse causality means that if we have changes in $Y$, this will also cause changes in $X$.
#>

We summarize that we have a biased estimator and reveal causality. This means, that we are confronted with endogeneity.

#< info "Endogeneity and Exogenity"
*Endogeneity* means, that the is relationship between the explanatory variable and the error term. The explanatory variable collerates with the residuals. Normally, all observable factory will be part of the error term of the regression. Thus, a variable which correlates with the explanatory variable will be as well included in the error term. Thus the explanatory variable correlates with the error term.

The opposite is called *Exogenity*. If we have exogenity, the estimator is consistent.
#>

The conclusion is, that we have to adapt our equation

$$Y_{ct} = \gamma_{t}+\gamma_{e}C_{ct}/P_{ct}+U_{ct}$$

to get rid of endogeneity. Note that I renamed $\beta_{1}$ to $\beta_{e}$ to point out that we have endogeneity, and the error term $\epsilon$ to $U_{ct}$.

Before introducing how we could solve this problem, we will first introduce the minimum capital stock requirement and check how it influences the capital stock.

## Exercise 3.2. - Capital Stock and Minimum Requirement: An Overview

A conclusion of the National Banking Act 1863 was the minimum capital stock requirement of the size `50.000$`. If a bank wanted to enter at a county, they needed a capital stock amount of `50.000$`. Thus, our explanatory variable $C_{ct}/P_{ct}$ depends on this rule. If a national bank had less than `50.000$` capital stock for profit maximization, they didn't enter at all or entered with the minimum capital. Thus $C_{ct}$ is either `0` or `50.000` in that case.

In the exercise before we estimated our equation
$$Y_{ct} = \gamma_{t} + \gamma_{e}C_{ct}/P_{ct}+U_{ct}$$
without respecting the minimum capital stock requirement. All banks included in our estimation weren't constrained by that rule. Thus we calculated the unconstrained effect of the entry of national banks in a county. The production per person increased by `6.12%`, when opening a national bank to the county. In the following we want to include the minimum capital stock rule to our analysis. 


#### Get an Overview of the Data Set

Read in the dataset. 

**Task:** 

Read in the data set `firstTable.tab` and save it under `tab1`. Use as separator `sep="\t"` and don't forget to set the `header` as `TRUE`.  Replace `??` and `#`. 

```{r "1_1___Minimum_Capita__3"}
#< task
# ?? 
#> 
#< hint
display("Remember the form the function read.table(data, sep, header)")
#>
tab1 = read.table("firstTable.tab", sep="\t", header=TRUE)
```

#< award "Read in a table Level 2"
Awesome! You wrote the code for reading in data sets all on your own.
#>

First, we want to take a closer look at our data set. One way is to check it out under `Data Explorer`, but there is also a huge amount of calls in R, which can give us a lot of information. With `dim()` we will get the dimension of the data, meaning how many columns and how many entries the data has, and with `head()` we will see the first few rows. Let's try it out and click on `check`:

```{r "1_1___Minimum_Capita__4"}
#< task_notest
dim(tab1)
head(tab1)
#>
```

Great! We have seven columns with 22552 entries in our table. 

Take a look at the second row of the displayed table. There we see, that in `1870` the national banks in the county `Autauga` in `Alabama` had a capital stock of `0`. That means that at this point of time, there hasn't been a national bank in `Gautama`. They hadn't enough capital to afford a national bank.
If a bank in a county has less than `50.000$` capital stock, it is marked with `0` in our data set. Some values are marked as `NA`, meaning that we don't have any data for this case. The column `capitalstock1902` is only interesting for us, if we look at the year `1900`. In this year the minimum capital stock requirement got reduced from `50.000$` to `25.000$`. So, the column `capitalstock1902` shows us how the capital amount is in `1902`. Since this new rule was introduced in 1900, the data of the years from 1900 are still valid for the old requirement. The changes caused by the new rule can be seen by the data of 1902. For `Autauga` the new requirement didn't make a change, they still couldn't entertain a national bank. 

With `sample_n()` we get a random selection of rows of our data, to get even more familiar with our data set.

**Task:** 

Use the function `sample_n()` in order to show a random sample of ten rows of our table `tab1`. With `set.seed(x)` you can create a random variable, on this way it is possible to reproduce the same sample all the time.

```{r "1_1___Minimum_Capita__5"}
#< task
#set.seed(1900)
#sample_n(??,10)
#>
set.seed(1900)
sample_n(tab1, 10)
```

Now, you can see a random choice of rows of our data set `tab1`. Get familiar with the output and try to answer the next questions.


**Question**

#< quiz "Ohio"
question: Did the capital stock to open a national bank in Highland in the state Ohio change after introducing the new lower minimum capital stock requirement of 25.000$?
sc:
- yes
- no*
success: Correct! The amount of available capital stock didn't change in Highland. Thus we can assume, that Highland wasn't constrained by the minimum requirement.
failure: Try it again.

#>

**Question**

#< quiz "Kentucky"
question: Do you think, that the banks in Ballard in the state Kentucky were constrained by the minimum capital stock requirement of the size 50.000$?
sc:
- yes*
- no
success: Correct! Since the capital amount changed from 0 to 30 (compare the value of capital stock and capitalstock1902), we can assume that the banks were constrained by the `50.000$` requirement since reducing it to `25.000$` lead to more entering of national banks.
failure: Try it again. Compare the the value of capitalstock and capitalstock1902.
#>


In the next step, we want to know how many states and how many counties are in our data set. Therefore we will make use of the function `unique()` and `length()`. 

`unique(x)` produces a vector with unique values of `x`, all duplicated elements will be removed. `length(y)` generates the length of vector `y`, meaning it calculates how many elements are available in `y`. 

**Task:**
Use `unique()` and `length()` to find out how many `states` and `counties` are in our data. Delete `#` and replace `??`. After finishing this task, try to answer the question.

```{r}
#< task
# ??(??(tab1$statename))
# ??(??(tab1$countyname))
#>
length(unique(tab1$statename))
length(unique(tab1$countyname))
```

#< award "Data Overview"
Great! You learned some ways to get an overview of a data set and can use them now on your own.
#>



**Question:**

#< quiz "countynum"
question: How many counties are presented in our data set? Type in an integer.
answer: 1649
#>



#### How did the number of national banks change between 1870 and 1900?

To analyse this, we need the data from 1870 to 1900. To reduce the size of our sample we will just use counties with a capital stock less than 200.000 $. Since the values in our data frame are given in 1.000 Dollar, we must filter out `capitalstock <= 200`. For this we make use of the function `filter` from the package `dplyr`.

#< info "filter()"
The function `filter(dat, cond)` from the package `dplyr` gives us certain rows of a data frame. Therefore, we have to define the condition `cond` by which the data set `dat` should be filtered.
If we want to select a certain amount of columns, we can do this with `select(dat,cond)` also from the package `dplyr`. 
To use this function, we have to load the package with `library(dplyr)` first.

To learn more about that, follow:

"https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html" 
#>

**Task:**

Try to find all counties from our data `tab1`, which are documented at the period `1870 - 1900` and which had a `capitalstock` less than `200`. For this you need to use the function `filter`, therefore first load the package `dplyr`.

```{r "1_1___Minimum_Capita__6"}
#< task
#library(??)
#dat1 = ??(??, year>= ??, year <= ??, ?? <= 200) %>% mutate(Year = as.factor(year))
#>
library(dplyr)
dat1 = filter(tab1, year>= 1870, year <= 1900, capitalstock <= 200)%>% mutate(Year = as.factor(year))
```

Well done! 
`mutate()` is also a function from `dplyr`. It helps us to add new variables to our data set. So, in the task before, we also created the column `Year`. As we set it as `as.factor(year)` we defined it as a factor column. 

Let's take a quick view on how this new variable `Year` looks like. Press `check`.

```{r}
#< task_notest
set.seed(1234)
sample_n(dat1, 10)
#>
```

As you recognize, the column `Year` has the same values as `year`. The difference is, that in `year` the values are saved as integers and in `Year` we saved them as factors. We will need this later when plotting. 


#< info "%>% Operator" 
The pipe operator `%>%` of the package `dplyr` transmits the output from one function to the input of the next function.Thus you can easily read the function from left to right instead of reading it from the inside to the outside.
#>



We will visualize the size of the capital stock over the years now. A useful package to plot this is `ggplot2` 

#< info "Package ggplot2" 
The package `ggplot2` is a helpful tool to create graphics, especially for plots belonging to the same group. The basis of this package is the function `ggplot()`. 
```{r eval=FALSE}
ggplot(data, aes(x=x_coordinate, y=y_coordinate, 
                 fill=factor)
```
First, `ggplot` needs mandatory a data argument. Afterwards you can specify more details, for example `aes()`. It modifies the aesthetics. Here it is also possible to do some settings for the elements of the plot. E.g. with `fill` you can set after which factor the data points should be filled. 

`ggplot()` represents a diagram as a list and saves it as an object. It is the lower layer of a diagram. (Wollschlaeger, 2014)
With `+` we can add more layers to our diagram. On this ways, we can create a specific plot and specify some settings for the plot.

Here are some examples to plot a special graph. (Maindonald, Braun, 2010, p.491) 
<style>
.mytable {
  border-collapse: collapse;
  margin-bottom: 15px;
}
.mytable td {
  padding: 10px 15px;
}
.mytable th {
  padding: 10px 15px;
  border-bottom: 1px solid #ddd;
}
.right-border {
  border-right: 1px solid #ddd;
}
.bigger-font-size {
  font-size: 18px;
}
</style>
  
  <table class="mytable">
  <tr>
  <th class="right-border">geom=</th>
  <th>ggplot()</th>
  <th>Available arguments</th>
  </tr>
  <tr>
  <th class="right-border">"point"</th>
  <th>+ geom_point()</th>
  <th>size, shape,..</th>
  </tr>
  <tr>
  <th class="right-border">"line"</th>
  <th>+ geom_line()</th>
  <th>size, linetype</th>
  </tr>
  <tr>
  <th class="right-border">"density"</th>
  <th>+ geom_density()</th>
  <th>weight, linetype, size</th>
  </tr>
  <tr>
  <th class="right-border">"histogram"</th>
  <th>+ geom_histogram()</th>
  <th>linetype, weight</th>
  </tr>
  </table>
  
  
Some further layers can specify the plot setting:

- `ggtitle("title")` : title of the plot
- `labs(x="text", y="text")` : label of the x and y axis
- `facet_wrap(~factor)` : makes it possible to plot facets next to each other arranged as a rectangle, `factor` stands for the variable you want to split up your data
  
All in all a plot with the package `ggplot2` can look as following:

```{r eval=FALSE}
library(ggplot2)
ggplot(data, aes(x=year, y= number of banks, fill=Banks)) + geom_point() + labs(x= "Year", y= "Number of Banks") + facet_wrap(~Banks)

```

  
To read more about the possibilities of plotting graphs, follow this link:

 https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf. 
#>


**Task:**

We want to plot our data `dat1` into a histogram to compare the amount of national banks over the years. We want to plot facets by using the factor-variable `Year`. `breaks = seq(..)` specifies the size settings of our histogram. Delete `#` and replace `??`.

```{r "1_1___Minimum_Capita__7"}
#< task
#??(data=??, aes(x=capitalstock, fill=year)) + ??(breaks=seq(0, 200, by =10)) + labs(x="County nationalbank capital stock ", y= "Number of counties") + facet_wrap(~Year)
#>
ggplot(data=dat1, aes(x=capitalstock, fill=Year)) + geom_histogram(breaks=seq(0, 200, by =10)) + labs(x="County nationalbank capital stock ", y= "Number of counties") + facet_wrap(~Year)
```
#< award "Histogram"
Congratulations! You can plot now a nice histogram and know how to use facets. 
#>

In the plot we can see that a lot of national banks in certain counties had a capital stock of 0. Thus, they had no entry of national banks in this year. Furthermore, we recognize a bunching at 50, meaning that a lot of national banks entered with the minimum capital stock. We don't know if theses banks opened with their optimal capital amount for profit maximization, or if they opened with even a higher capital amount compared to what they would do, if no restriction were there. This leads us to the suggestion that the minimum capital requirement was constraining. 


1900 the minimum capital was reduced from 50.000 Dollar to 25.000 Dollar. How did this affect the behavior of national bank entries? As described before, in our table `tab1` we have data from 1900 and 1902. Now we want to compare them to answer the question. Therefore, we first have to filter our data and then plot them. I give you the code for that, just press `check`.

```{r "1_1___Minimum_Capita__8"}
#< task_notest
# Filter data `tab1`
cap1900 = filter(tab1, capitalstock <=200, capitalstock1902<=200, year==1900)
# Plot two histograms, one for the data of 1900 and one for the data of 1902
ggplot(data=cap1900)+ geom_histogram(aes(cap1900$capitalstock), breaks=seq(0, 200, by =3),col ="blue", fill="blue", alpha = .5) + geom_histogram(aes(cap1900$capitalstock1902), breaks=seq(0, 200, by =3),col ="green", fill="green", alpha = .5)+ labs(x="County national bank capital stock", y= "Number of counties")
#>
```

The blue filled bars show the capital stock of 1900, the green bars the capital amount in 1902.
Try to answer following questions, according to the diagram:

**Question:**

#< quiz "Plot1 "
question: Did the number of unbanked counties decreased or increased from 1900 to 1902?
sc:
    - increased
    - decreased*

success: Correct! 1902 a lot of more counties got a bank compared to 1900.
failure: Try it again.
#>


The number of unbanked counties (with capital stock 0) decreased a lot and a huge amount of national banks opened with 25.000$ capital stock, the new minimum size. We can assume that many counties were willing to support a national bank but didn't had that much capital stock. Thus, they could have a smaller bank before but didn't had one because of the required minimum.

Furthermore, the number of counties with national banks with a capital stock of 50.000$ increased as well. Many reduced their capital amount.

**Question**

#< quiz "Plot2"
question: Regarding these facts, do you think the minimum capital stock constraint was binding?
sc: 
    - yes*
    - no
succes: Yes. Regarding all these facts, the minimum capital stock constraint must have been binding.
failure: Try it again! 
#>

Resume of the facts, why minimum capital stock was binding:

1. We have a bunching of banks with capital amount of 0 or 50.000$ capital amount.
2. Reducing the minimum size to 25.000$ leads to less bunching of banks with capital amount of 0.
3. Reducing the minimum size to 25.000$ leads to more bank openings with capital amount o 25.000$. 

## Exercise 3.3. - Effect of Constrained Banking on Production

#### Introduction to Optimal Capital Stock of a Bank

In the last exercise you got an overview of our data set. Furthermore, we saw that the minimum capital stock requirement was binding for banks. They couldn't just open a bank with their optimal capital amount, they were constrained by the rule. 
But how is *optimal capital* for a national bank defined?

Like Varian (2011, Chapter 1) explains, every bank tries to choose the best option to enter in a county, which the bank can achieve. We will define the optimal capital stock for a bank in a certain county at a specific time as following:
  
$$C^{*}_{ct}=\alpha\alpha_{t}P^{\theta+1}_{ct}\eta_{c}\epsilon_{ct}$$
    
where $P_{ct}$ displays the county population, $\eta_{c}$ stands for local unobservant business conditions and $\epsilon_{ct}$ for local temporal shocks.
  
#< info "How to get the optimal capital stock equation for banks"
How do banks make profits? 
- taking deposits $D=K-C$
- lend capital $L$
    
While lending capital, the bank can charge an interest quote $r_{L}(L)$ and has costs of $r_{L}$. To afford these loans, they must raise capital $K$ equal to $L$. Similarly, for raising capital stock $C$ and deposit $D$ they have to pay $r_{C}$ and $r_{D}(D,C)$. Thus:

$$max_{C,K}r_{L}(K)K-r_{c}C-r_{D}(K-C,C)(K-C)$$

s.t. $0<= C <= K$.

Through the first order condition we will find $K^*, C^*$ and the optimal equity-asset-ratio $\omega^*=C^*/K^*$. In county $c$ at time $t$ with population $P_{ct}$ we can define 
$$r_{L}(L)= \alpha_{0}\alpha_{t}P_{ct}^{\theta+1}\eta_{c}\epsilon_{ct}-\alpha_{1}L=\alpha_{ct}P_{ct}^{\theta+1}-\alpha_{1}L$$
where deposit cost for a bank is $\rho_{D}-\xi$, if the bank has just capital. But if the deposit raises, they become more expensive and thus $\omega$ decreases. Total costs of capital can be represented via
$(\rho_{D}-\xi_{\omega})(1-\omega)K+r_{C}\omega K$.

Only the capital structure can affect the cost of capital, so minimizing the cost gives $\omega^*=(\rho_{D}-r_{C}+\xi)/(2\xi)$. Given these optimal costs of capital, the bank chooses the size to correlate the cost of raising capital with the return from lending it. 
Respecting all these conditions we will get:
    
$$C^*_{ct}= (\alpha_{0}\alpha_{t}P_{ct}^{\theta+1}\eta_{c}\epsilon_{ct})/(2(\alpha_{1}+\rho_{\omega})/\omega)=\alpha\alpha_{t}P_{ct}^{\theta+1}\eta_{c}\epsilon_{ct}$$
where $\alpha$ returns all of the other model parameters and $\rho_{\omega} = r_{c}\omega+(\rho_{D}-\xi_{\omega})(1-\omega)$.
If you want to go deeper in this derivation, you can read chapter `S.1` from the online appendix:
    
https://www.mitpressjournals.org/doi/suppl/10.1162/REST_a_00546/suppl_file/REST_a_00546-esupp.pdf
#>
  
Under the assumption that banks will always open with their optimal capital stock, we redefine the outcome of interest as: 
    
$$Y^{*}_{ct} = \gamma_{t} + \gamma_{e}C^{*}_{ct}/P_{ct}+U^{*}_{ct}$$ 
  
- $Y^{*}_{ct}$: outcome of interest, in our case that will be production per person
- $P_{ct}$: county population at the time $t$ in county $c$
- $C^{*}_{ct}$: optimal capital stock
- $U^{*}_{ct}$: error term
- $\gamma_{t}$ , $\gamma_{e}$: specific effects

<!-- In our regression model  -->

<!-- $$Y^{*}_{ct} = \gamma_{t} + \gamma_{e}C^{*}_{ct}/P_{ct}+U^{*}_{ct} (1)$$ -->

<!-- we have an endogeneity problem. The variable $C^{*}_{ct}/P_{ct}$ is an endogenous explanatory variable, meaning it correlates with the error term $U^{*}_{ct}$. Thus, we will receive an inconsistent and biased estimator $\hat{\beta}$, because $\mathbb{E}((C^{*}_{ct}/P_{ct})'U^{*}_{ct}) !=0$ (Winker (2007), p.177).  -->

<!-- As the aim of every economist is to deduce that one variable has a causal effect on another variable, we must solve this endogeneity problem. -->

During the observed period from 1870 to 1900, national banks couldn't freely enter in a county. They had to open with a minimum capital stock of at least 50.000$. Differentiate between the size of capital stocks of banks:

- $C^{*}_{ct}$: capital stock size to optimize the profit of a bank

- $C_{ct}$: actually capital stock size of a bank which entered in a county

We get the following two cases.

1. $C^{*}_{ct} >= 50.000$ Dollar: If the optimal capital stock for a bank is higher than the required minimum capital stock, it is obvious that the bank will open with the profit-maximizing capital. Thus, $C_{ct} = C^{*}_{ct}$  

2. $C^{*}_{ct} < 50.000$ Dollar: In this situation a bank can either denier to enter in a county or were forced to open with the minimum capital of `50.000$`. Thus, $C_{ct}$ jumps discontinuously between `0` and `50.000$`. We have either $C_{ct}=0$ or $C_{ct}=50.000$.
  
  
How can we calculate the optimal capital stock for our data set? Fulford (2015, p.926-927) explains that the optimal capital stock can be estimated by a maximum likelihood estimator. The results of this estimation are already included in our data set.

#### Efect of Constrained Banking on Production

Like in Exercise 3 we will estimate our regression model, but this time we respect the minimum capital stock of 50.000$ and include the assumption, that banks always try to enter with their optimal capital stock.

$$Y^{*}_{ct} = \gamma_{t} + \gamma_{e}C^{*}_{ct}/P_{ct}+U^{*}_{ct}$$

First we have to read in our data again. Press `edit` and `check`. 

```{r}
#< task_notest
Dat1 = read.table("Data2.tab", header = TRUE, sep = "\t")%>%
       mutate(capitalstock50pc = capitalstock_pc*14755 / 50, insample = insample_allrural & tpop>1000, L.capitalstock = lag(capitalstock))
#>
```

Using the `mutate()` function, we created the variable `capitalstock50pc`. It is the capital stock normalized by the average population of 14.755 during the observed period in a county. Furthermore, to make it easier to read, we measure this variable in units of `50.000$`. So it will be in units of adding a bank of minimum size to the county population.
We define `insample` to select all rural counties, and also we will just pay attention to counties with a population greater than 1000, `tpop>1000`. With `lag()` we define the lag variable for capitalstock, to respect growth effects.

To calculate the effect of constrained entry of a national bank to a county, we will include the minimal capital stock requirement of `50.000$`. We will compare counties that don't have a bank to counties with a bank, which opened at the minimum size. 
To get this effect, it is necessary to filter out all counties with a capital amount of less than `50.000$`. On this way we get all counties with national banks with a capital size of either `0` of `50.000$`.

**Task:** 

Filter `capitalstock<=50` and `capitalstock50pc<=5`, to get all constrained banks. Again, our data selection should be in our defined `insample` variable, remember that this shows us all rural counties with population greater than 1000. 

```{r "2_1___Estimation_a_5"}
#< task
#d1 = filter(Dat1, insample, ??, ??)

#>
d1= filter(Dat1, insample, capitalstock<=50, capitalstock50pc <= 5)
```
#< award "dplyr"
Congratulation! You now how to use the functions out of the dplyr package.
#>

You already know, how to regress an equation by using the function `lm()`. With the function `felm()` out of the package `lfe`, I will show you a much faster way to estimate linear model. 

#< info "felm() function"
`felm()`is a function of the package `lfe` to estimate linear models. The goal here is that we can separate the effects from the normal equation. Therefore, we can use it for huge data sets. It's defined as: 
```{r eval=FALSE}
felm(y ~ x1 + x2 | f1 + f2 , data = dataname)
```

`y ~ x1+x2` is the regression input, `f1+f2` are the multiple fixed effects.
#>

**Task:**

Estimate `ln_tval_pc~capitalstock50p` using the fixed effect `year` on the base of the data set `d1`. Use `felm()`, but first load the package `lfe`.

```{r "2_1___Estimation_a_4"}
#< task
#library(??)
#summary(??(ln_tval_pc~capitalstock50pc |year , data = d1))
#>
library(lfe)
summary(felm(ln_tval_pc~capitalstock50pc |year, data = d1))
```
#< award "Regression Level 2"
You performed the first linear regression using `felm()`. Congratulations!
#>

Well done! You already know how to interpret the `Coefficients` part of a `summary()` output. For the sake of completeness, I will explain the whole output in the following info box.

#< info "Output of summary()"

<style>
.mytable {
  border-collapse: collapse;
  margin-bottom: 15px;
}
.mytable td {
  padding: 10px 15px;
}
.mytable th {
  padding: 10px 15px;
  border-bottom: 1px solid #ddd;
}
.right-border {
  border-right: 1px solid #ddd;
}
.bigger-font-size {
  font-size: 18px;
}
</style>
  
  <table class="mytable">
  <tr>
  <th class="right-border">Call </th>
  <th>`Call: felm(formula = ln_tval_pc ~ capitalstock50pc | year, data = Dat_A)`
displays our regression call.</th>
  </tr>
  <tr>
  <th class="right-border">Residuals</th>
  <th>Residuals are the difference of the observed response value and the predicted value of the model. They can help us to check, whether our model is good enough. Therefore, they should be normal distributed with mean value around `0`. To check this, following is necessary: The median should be around `0`, and the first and third Quantile should be symmetric about `0`. In our case these conditions are true.</th>
  </tr>
  <tr>
  <th class="right-border">Coefficients</th>
 
  <th>In this part you will find the values of the regression-equation. 

*Estimate:*  this is the slope if we add a minimum size bank to a county. 

*Std.Error:* the standard error measures the average amount that the coefficient estimates differ from the actual average value of the explanatory variable.

*t value:* shows how much standard deviation the estimated coefficient is away from `0`. If the value is far away from `0`, it's better because than we could reject the Null-Hypothesis. If this is the case, we can assume that there is a relationship between our log-production per person and the capital stock amount. 

*Pr(>|t|):* here we find the p-value for a T-Test. Our p-value is smaller then the significance value `0.001`. Thus we can reject the Null-Hypothesis again.</th>
  </tr>
  <tr>
  <th class="right-border">Residual standard error</th>
  <th>Here we get information about the variance of the residuals.</th>
  </tr>
  <tr>
  <th class="right-border">Multiple R-squared</th>
  <th>Shows how good the regression fits in the data.</th>
  </tr>
  <tr>
  <th class="right-border">F-Statistic</th>
  <th>Measures the relationship between `ln_tval_pc` and `capitalstock50pc`. The higher this value is, the better the relationship is.</th>
  </tr>
  <tr>
  </table>

#>

**Question:**

#< quiz "Output"
question: How much more output per person will be reached by adding a minimum-size bank to a county, base on the last results? 
sc:
    - 34.93
    - 21.80*
    - 13.29
    - 1.64
success: Correct!
failure: Try it again.

#>

#< info "t-Test for the regression coefficient"
We will introduce the *t-Test* for the regression coefficient $\beta$ to get a better knowledge about the summary-output. The important line for our analysis is the one of the `Coeffcients` part, here `capitalstock50pc`.

We want to check whether there is a linear relationship between our variable for sure.

Zero-Hypothesis $H_{0}$: $\beta = 0$ 

Alternative-Hypothesis $H_{A}$: $\beta >0$ or $\beta <0$

If $H_{0}$ holds, than there is no linear relationship between the dependent variable and the explanatory variable of our regression. If $H_{0}$ can be rejected, $H_{A}$ holds and there is a relationship between the variables. The sign of $\beta$ shows the direction of the relationship. 

$t = b / se_{b}$ follows a t-distribution under the Zero-Hypothesis. $b$ is the regression coefficient and can be found under `Estimation`, here $b=0.2180$. $se_{b}$ is the standard error of $b$, here $se_{b}=0.0164$. Thus $t=0.2180/0.0164=13.29$, the same we find under `t value`in our output. Perfect! 

Now check `Pr(>|t|)`. It gives us the p-value for our test. Since $Pr(>|t|) = 2e-16 > 0$ we can reject the Zero-Hypothesis. Thus, we have a linear relationship between our variables of the regression (ct. Hatzinger, Hornik, Nagel (2011), pp.285).
#>

As I introduced how a linear model is general estimated, I explained you that the estimation is based on the `OLS Estimation`. In the following you can learn more about that topic.

#< info "Ordinary Least Squares Estimation"
Consider the multiple regression model from before:

$Y = \alpha + \beta_{1}X_{1} + ... + \beta_{n}X_{n} + \epsilon$

For a better overview, we will now rename $\alpha$ to $\beta_{0}$. Furthermore we can than compress $\beta_{0}, \beta_{1},..., \beta_{k}$ to a vector $\beta$ containing the true coefficient and $X_{1},..., X_{n}$ to a matrix $X$. Thus our regression model is:

$$Y = \beta X + \epsilon$$
$\hat Y = \hat{\beta} X$ is the estimator of $Y$. Thus, we can defined the standard error tern as $\epsilon = Y - \hat{Y}$.

The OLS estimator $\hat{\beta}$ is determined to minimize the objective function $S(\hat{\beta})$, which is defined by
$$
S(\hat{\beta}) = \sum_{i=1}^k \epsilon_i^2 = \sum_{i=1}^k (Y_i - \hat{Y}_i)^2
$$
where $k$ displays the number of observations. (c.t. Wooldrige (2016), Chapter 3).

Like Auer and Rottmann (2010, p, 420) explain, the OLS $\hat{\beta}$ must minimize $S(\hat{\beta})$, the sum of squared residuals. Thus
$$\hat{\beta} = (X'X)^{-1}X`Y $$
The aim of the estimation is to get an optimal estimation for $\hat{\beta}$.
#>

When estimating a regression model we want to achieve an optimal result. Thus, some assumption has to be fulfilled, like Kennedy (2008, p.41-42) explains in his book.

#< info "Five Assumptions for an optimal OLS estimator"

1. The linear model $Y$ can be calculated as a sum of independent variables and a error term.

2. The expected value of the error term of the model is zero, $\mathbb{E}(\epsilon)=0$.

3. The disturbances have the uniform variance and aren't correlated with each other, $\mathbb{E}(\epsilon\epsilon') = \sigma ^2 I$.

4. In repeated samples, the observations of the independent variable can be consider as fixed.

5. The number of observations is greater than the amount of independent variables.


Under this assumption we have the best OLS. Furthermore, we can than claim that the OLS estimator is even the **best linear unbiased estimator** (Auer, Rottmann(2010), p.456). That means, the OLS estimator is 

1. *linear*

2. *unbiased* and *consistent*: OLS is close to the true value and the average is the true value, $\mathbb{E}(\epsilon)=0$

3. *efficient*: $\hat{\beta}$ has among all linear estimators the minimal variance 

#>

Since we have endogeneity in our model, we break the second assumption cause we have a biased estimator.

The endogenous effect of unconstrained entry can also be estimated by using an *indicator* for whether the county has a bank or not.

**Task:**

First create a new data set `d2`, based on our data set `Dat1`. Here we want to take counties with a `capitalstock <= 5` and counties which were included in the predefined value `insample`. Afterwards create a regression where we compare the relationship between `ln_tval_pc` and `banks50c`. Include `year` as fixed effects.

```{r "2_1___Estimation_a_6"}
#< task
#d2 = ??(Dat1, ??, ??)
#summary(??(ln_tval_pc ~ ?? | ??, data=d2))
#>
d2= filter(Dat1, insample, capitalstock<=50)
summary(felm(ln_tval_pc ~banks50c | year, data=d2))
```
#< award "Regression Level 3"
Awesome! You created a regression on your own and can call yourself a Master of Linear Models. Furthermore you got used to interpret the results of such regressions.
#>

Production per person gets `37%` higher if we use the presence of banks as an indicator.

#< quiz "NullHypothesis"
question: Can we reject the Null-Hypothesis in our last regression?
sc:
    - yes*
    - no
success: Correct!
failure: Try it again.

#>


#< quiz "NullHypothesis2"
question: What does it mean, if we can reject the Null-Hypothesis?
sc:
    - there is no relationship between the response (ln_tval_pc) and the predictor variable (capitalstock50pc)
    - there is a relationship between the response (ln_tval_pc) and the predictor variable (capitalstock50pc)*
success: Correct!
failure: Try it again.

#>


#### Fixed effects

We already involved time effects in our regression equation, yet we also want to include county fixed effects. Our regression model is now of the form

$$Y^{*}_{ct} = \gamma_{t} + \gamma_{e}C^{*}_{ct}/P_{ct}+U^{*}_{ct}$$

To calculate estimations with fixed effects R offers also the package `plm`, a linear model for panel data. We will use the function `plm` out of this package. This function uses the known `lm` function to estimate panel data. But with it we can set estimation models like pooled OLS "pooling", first difference "fd" or fixed effects "within" of which we will make use. The structure of a `plm` function is:


plm(formula, data, subset, weights, na.action, effect = c("individual", "time", "twoways", "nested"), model = c("within", "random", "ht", "between", "pooling", "fd"))

If you want to learn more about the `plm` package look through the following link:

"https://cran.r-project.org/web/packages/plm/plm.pdf".

In the last exercises we made three estimations with time effects, to check how the adding of a bank to a certain county affects the production per person there. Our result was, that if we add a bank at minimum size to a county, we receive a higher output per person. There we differed between counties which had already a bank and counties without any bank before.
Now we want to include county fixed effects as well, thus all fixed effects of our estimation are included. To do this we use `plm`. 
Let's start with our first regression, where we just use counties that have at least one bank.

**Task**

Use the `plm`-function to estimate the effect between `ln_tval_pc` and `capitalstock50pc`. Since we want fixed effects to be included, use `model="within"` and `effect="twoways"`. Make use of our predefined data set `d1`.

```{r "2_1___Estimation_b",results='asis'}
#< task
#library(plm)
#plm1 =(??(??~ capitalstock50pc + factor(year), model="??", effect="twoways",data = d1))
#stargazer(plm1, type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE))
#>
library(plm)
plm1 = plm(ln_tval_pc~ capitalstock50pc + factor(year), model="within", effect="twoways", data =d1)
stargazer(plm1, type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE)
```
#< award "Regression Level 4"
Great! You created your first regression by using the plm-function.
#>

#< info "stargazer()"

With `stargazer()` from the package `stargazer` it is possible to present the results of various models in one table.

```{r eval=FALSE}
#Load package
library(stargazer)
#Compare results of regressions
stargazer(reg1, reg2,..)
```

The structure of the table can be specified by many options:

- *type*:  A character vector indicating the type of output the table should have. Possible values are html, text and latex.

- *digits*: Integer of decimal places that should be displayed

- *report*: Choose the statistics that should be displayed in the table, variable names (v), coefficients (c), standard errors (s), test statistics (t) and p-values(p). For adding significance stars, put an (*) after the character.

- *omit state*: Choose the statistics that shouldn't be displayed in the table, e.g. `ser`for standard errors.

- *model.number*: Logical vector which specifies if the models should be numbered.

- *column.labels*: Names the columns in the regression table.
#>

Including fixed effects leads to a higher output per person for about `2.4%` when adding a minimum size bank to a county with at least one bank. Comparing the result to our result without fixed effect of `6%`, that means much less production per person. This can be explained by respecting that "banks are attracted to more productive areas", like Fulford says. 

In the next step we will estimate how much more output a county will get by adding a bank, independent if they had a bank before or not. We will include fixed effects and then compare our result to the one without any effects. I will use again `plm` for the estimation and `stargazer` to compare the results. 

**Task**

Delete `#` and press `check`.

```{r "2_1___Estimation_b_2",results='asis'}
#< task
#stargazer(plm(ln_tval_pc~ capitalstock50pc + factor(year), model="within", effect="twoways", data =d2), felm(ln_tval_pc ~capitalstock50pc | year, data= d2),type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE)
#>
stargazer(plm(ln_tval_pc~ capitalstock50pc + factor(year), model="within", effect="twoways", data =d2), felm(ln_tval_pc ~capitalstock50pc | year, data= d2),type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE)
```

**Question:**

#< quiz "plm"
question: How much higher is the production per person output using fixed effect? Type in a decimal with one degree after the point.
answer: 17.8
#>

Counties with banks are more productive, than counties without banks. Respecting the fixed effects, production per person is `17.8%` higher.

In the last step we repeat this procedure to our last regression.

**Task**

Delete `#` and press `check`

```{r "2_1___Estimation_b_3",results='asis'}
#< task
#stargazer(plm(ln_tval_pc~ banks50c + factor(year), model="within", effect="twoways", data =d3), felm(ln_tval_pc ~banks50c | year, data= d3),type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE)
#>
stargazer(plm(ln_tval_pc~ banks50c + factor(year), model="within", effect="twoways", data =d3), felm(ln_tval_pc ~banks50c | year, data= d3),type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE)
```

Using an indicator for banks and including fixed effects, counties with a bank will have around `11.5%` more production per person than counties without a bank. Since endogeneity is important we must include the fixed effects, which leads to lower estimate results. 

Perfect! We have already seen that gaining a national bank helps a county to get a higher production per person. I will summarize our results in the following table. Be aware that they all show the endogenous effect of entry.

<style>
.mytable {
  border-collapse: collapse;
  margin-bottom: 15px;
}
.mytable td {
  padding: 10px 15px;
}
.mytable th {
  padding: 10px 15px;
  border-bottom: 1px solid #ddd;
}
.right-border {
  border-right: 1px solid #ddd;
}
.bigger-font-size {
  font-size: 18px;
}
</style>
  
  <table class="mytable">
  <tr>
  <th class="right-border">Regression Model</th>
  <th>Capital per capita</th>
  <th>Year FE</th>
  <th>County FE</th>
  </tr>
  <tr>
  <th class="right-border">Unconstrained Entry</th>
  <th>6.12</th>
  <th>Yes</th>
  <th>No</th>
  </tr>
  <tr>
  <th class="right-border">Unconstrained Entry</th>
  <th>2.42</th>
  <th>Yes</th>
  <th>Yes</th>
  </tr>
  <tr>
  <th class="right-border">Constrained Entry</th>
  <th>21.8</th>
  <th>Yes</th>
  <th>No</th>
  </tr>
  <th class="right-border">Constrained Entry</th>
  <th>17.8</th>
  <th>Yes</th>
  <th>Yes</th>
  <th class="right-border">Indicator for a Bank</th>
  <th>37.0</th>
  <th>Yes</th>
  <th>No</th>
  </tr>
  <th class="right-border">Indicator for a Bank</th>
  <th>11.5</th>
  <th>Yes</th>
  <th>Yes</th>
  </tr>
  <tr>
  </table>

## Exercise 3.4. - Solving the Endogeneity Problem

In our regression model 

$$Y^{*}_{ct} = \gamma_{c} + \gamma_{t} + \gamma_{e}C^{*}_{ct}/P_{ct}+U^{*}_{ct} (1)$$

we have an endogeneity problem. The variable $C^{*}_{ct}/P_{ct}$ is an endogenous explanatory variable, meaning it correlates with the error term $U^{*}_{ct}$. Thus, we will receive an inconsistent and biased estimator $\hat{\beta}$, because $\mathbb{E}((C^{*}_{ct}/P_{ct})'U^{*}_{ct}) !=0$ (Winker (2007), p.177). 

As the aim of every economist is to deduce that one variable has a casual effect on another variable, we must solve this endogeneity problem.

Fulford explains that the minimum size requirement leads to counties with too much banking and to other counties with too little. This effect can be measured by the *excess capital*. We define it in the following way:

$$\mathbb{E}(C_{ct})= C_{ct}-C^{*}_{ct}.$$
Excess capital will be positive, if a national bank enter with a capital size higher than their optimal capital stock. It will be negative, if these banks denier entry on minimum capital stock. 

Excess capital doesn't depend on the strategy of the banks, like the capital stock size in our regression model (1). Maybe this variable can help us to get exogeneity.

First, we will define the effect on the outcome of the minimum size regulation by using the excess capital. We take the difference between the outcome with the rule $Y_{ct}$ and the outcome without the rule $Y^{*}_{ct}$:

$$Y_{ct} - Y^{*}_{ct} = \gamma \mathbb{E}(C_{ct}/P_{ct})+U_{ct} (2), $$
where $\gamma$ displays the effect of the minimum size requirement. It measures the effect, if areas are forced to have more or less banking because of the requirement. In that sense, $\gamma$ is a casual effect. 

If we could now combine (2) in our regression model (1), we could estimate $\gamma$ as well es the endogenous effect $\gamma_{e}$. We will get the new equation:

$$Y_{ct} = \gamma_{c} + \gamma_{t} + \gamma_{e} C_{ct}^{*}/P_{ct} + \gamma \mathbb{E}(C_{ct}/P_{ct}) + e_{ct} (3)$$

To receive the causal effect of this equation, we have to know what banks actually want to do $Y^{*}$. But we can't observe this, thus we will normally still have endogeneity in the new regression model (3). Fulford explains in his paper how to handle this problem. 

> The standard sharp regression discontinuity result that $\gamma$ is identified hold as long as the density of $e$ conditional on $C^{*}$ is continous (Hahn, Todd & KLaaus, 2011; Imbens & Lemieux, 2008). A regression discontinuity does not require $C^{*}$ to be uncorrelated with $e$ since identificiation of the causal effect $\gamma$ is coming from the jump in $\mathbb{E}C$, which is a function of $C^{*}$. 
>

Under this assumption we can define the equation (3) as our new regression model, where $\gamma$ is the causal effect. Thus, we want now to regress log production per person on $C_{ct}^{*}/P_{ct}$ and $\mathbb{E}(C_{ct}/P_{ct})$. Our focus then will be on the estimator for the excess capital $\gamma$. In our next exercise, we try to estimate (3) by using the conditional mean.



## Exercise 5. - Estimation with Conditional Mean


In the last exercise we settled up our regression model as:

$$Y_{ct} = \gamma_{c} + \gamma_{t} + \gamma_{e} C_{ct}^{*}/P_{ct} + \gamma \mathbb{E}(C_{ct}/P_{ct}) + e_{ct}.$$
Our aim is to regress the effect of the minimum size requirement $\gamma$. Therefore, we have to calculate the excess capital, $$\mathbb{E}(C_{ct})= C_{ct}-C^{*}_{ct}$$ divided by the average population at the time $t$ in the county $c$. 


First, we calculate the mean optimal capital stock for each county-decade $C_{ct}^{*^{mean}}$ and then replace the unobserved optimal capital stock $C_{ct}^{*}$ with it. Then we can also calculate the mean excess capital $\mathbb{E}(C_{ct})^{mean}$ for each county-decade. Before, we have to check whether estimating on this way will lead to the right estimation of $\gamma$. 

What conditions do we have to fulfill?

1. Using the mean optimal capital stock shouldn't change the structure of the problem: Since the relationship between the capital stock per person and the output per person is linear, we will get a linear unbiased estimator $\gamma$ estimating with $C_{ct}^{*^{mean}}$.

2. The conditional mean must be unbiased: We define $C_{ct}^{*}= C_{ct}^{*^{mean}} + u_{ct}$, where $u_{ct}$ is the error term. If $E[u_{ct}|C_{ct}^{*^{mean}}]= 0$, the error term is independent of the observed value. Thus, it is correlated with the true value. Assuming this, replacing $C_{ct}^{*}$ with $C_{ct}^{*^{mean}}$ will still estimate $\gamma$ without bias.

With this assumptions, we can use conditional mean for our estimations.

Since our data set is huge and has a lot of missing values, I will show you how to estimate with conditional mean with a smaller sample. Afterwards I will present the results from Fulford for the whole data set. He uses multiple imputations to fill the missing values with simulated values. Then he calculates for each county and each decade the mean optimal capital stock and the mean excess capital and estimates the regression by using this means. If you are interested in the concrete way, please have a look at the Stata-Code from the file "National_bank_analysis3.do" which you can find here: 
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PQ6ILM.



We will just look at the state `Mississippi`, which contains only rural counties and was also one of the state under the CSA during the civil war. Have a look where Mississippi is located. Press `check`.

```{r}
#< task_notest
require(maps)
library(maps)
Mis = c("mississippi")
map(database = "state", col = c("white", "blue")[1+(map(database = "state", col = "blue",fill=T, namesonly=TRUE) %in% tolower(Mis) )],fill=T )
#>
```

First, we will not include information  about 1902 in our estimation. I calculated the mean of excess and optimal capital stock and normalized both, such that they are in units of adding a bank of minimum size to the county. 

**Task:**

Read in the data set `Mississippi.csv`. Find the mistake in the following code and delete `#`.

```{r}
#< task
# Mississippi = open.csv("Mississippi.csv", header=TRUE, sep="\t")
#>
Mississippi = read.csv("Mississippi.csv", header=TRUE, sep="\t")
```
#< award "Data Handling Expert"
You are able to detect wrong R-codes. Congratulations!
#>


Great! In the next step we will look at our new created mean variables. `Mopt_capital_pc` displays the mean of the optimal capital amount. Notice that the mean is calculated for each county and each decade. `Mexcess_capital_pc` shows the mean excess capital for each county and each decade. Excess capital displays the difference between the actual capital stock and the optimal capital stock, $\mathbb{E}(C_{ct})= C_{ct} - C_{ct}^{*}$. Press `check` to get a small overview of these variables.

```{r}
#< task_notest
set.seed(50)  #setting a seed to get always the same random sample
sample_n(select(Mississippi, year, countyname, Mexcess_capital_pc, Mopt_capital_pc, capitalstock50pc),10)
#>
```

Please have a look at the first line of our output. The national banks in the county `Prentiss` had in `1890` a capital stock of `0`, meaning there was no national bank at this time. Thus, the mean excess capital amount is $0-42900000 = -4290000$, as shown in the table. You can see, the shown counties didn't had a national bank during the displayed period. We want to check, how many counties had a national bank in Mississippi during the observed period. Be aware that some values are missing and that we have data for each decade from 1870 to 1900. 

**Task:**

`capitalstock50pc` shows the capital stock normalized by the average population of 14755 during this period in a county. Thus, if it is `0` we know, that a county didn't had a national bank. If it is something positive, we know that the county had national banks, which opened with a specific capital stock. Try to check how many counties in `Mississippi` didn't had a national bank in our sample. Use the variable `capitalstock50pc`.

```{r}
#< task
#??(Mississippi$capitalstock50pc)
#>
summary(Mississippi$capitalstock50pc)
```

**Question:**

#< quiz "NumberOfUnbankedCounties"
question: How many counties didn't have a national bank in our sample over different decades? Please type in an integer.
answer: 271
#>

In Mississippi many counties couldn't afford a national bank during the period of 1870 to 1900. Maybe they were restricted by the minimum capital stock size of 50.000$ or their optimal capital stock was far away of the requirement size..
Nevertheless, we will make estimations with conditional mean to check how extra banking could have effected the development in Mississippi.

We will do the estimations on basis of a one-period lag. On this way we can suggest continuing growth effects and not just single level effects. We must define the lags for our important variables `Mopt_capital_pc` and `Mexcess_capital_pc`. Furthermore, we will use in all regressions time and county fixed effects. Normally we would also cluster for states, but since we just look at the state Mississippi we just have one cluster and can skip this step.

Let's start to define the lag-variables. Just press `check`.

```{r}
#< task_notest
Mississippi = mutate(Mississippi, 
                     LMopt_capital_pc=lag(Mopt_capital_pc), LMexcess_capital_pc= lag(Mexcess_capital_pc))
#>
```


Perfect! Now we can start with the first estimation, where we want to check the log total production value per capital in Mississippi.

**Task:**

Create an estimation for `ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc` with `felm`. County and time fixed effects should be included. Delete `#` and replace `??`.

```{r}
#< task
#summary(??(ln_tval_pc ~ LMopt_capital_pc + ??|year+countynum, data=Mississippi))
#>
summary(felm(ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi))
```

Our aim was to calculate the mean excess capital to get an unbiased estimator. Thus, the important variable in this estimation is `LMexcess_capital_pc`. It shows us, how much extra banking would affect the production per person including a one-period lag. Thus, opening a national bank in Mississippi would increase production per person about `12.3%`.

What about `LMopt_capital_pc`? This variable displays the mean optimal capital stock over all decades and all counties in Mississippi. In our estimation it shows the impact of optimal capital stock per capita. Thus, the coefficient of the optimal capital per person in Mississippi is about `26.7%`. 

We checked how opening a national bank in Mississippi effects the total production per capita. But we can separate total production in manufacturing and agriculture. Let's check the effect of manufacturing production.

**Task:**

In the following I show you how to measure the effect of manufacture. Check the code and find the mistake.
```{r}
#< task
#summary(felm(ln_manuf_val_pc + LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi))
#>
summary(felm(ln_manuf_val_pc ~ LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi))
```

**Question:**

#< quiz "EffectManufacturing"
question: How much would the opening of a national bank on minimum size increase the manufacturing production per capita in Mississippi? Type in your answer. Use `.` as separator for decimals and use three digits after the point.
answer: 6.203
#>

Now we want to compare, how banking effects farm production. Press `check`.

```{r}
#< task_notest
summary(felm(ln_farmprod_val_pc ~ LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi))
#>
```

Gaining a national bank in Mississippi leads to a negative result for farm production in Mississippi. How could this be? First, remember that national banks couldn't lend land and the only way to help agriculture was to provide capital to farmers. As we have seen before there haven't been many national banks in Mississippi, thus farmer couldn't be promoted by them. Second, the manufacturing could have played a more important role in Mississippi than agriculture. Thus, national banks in Mississippi would have more support manufacturing production.

#### Reduced minimum capital stock from 50.000$ to 25.000$

In the whole analysis we didn't took notice of the reduced minimal capital stock requirement from 1900. Now, we want to include this information. Therefore, we read in a data set with information about 1902. Afterwards we will calculate the needed lags again. Remember that this new rule was published in 1900, but the effect of this change is documented in our data set in the year of 1902.

**Task:**

Press `check`.

```{r}
#< task_notest
#load file
Mississippi1902 = read.csv("Mississippi25.csv", header=TRUE, sep="\t")
#create new variables
Mississippi1902 = mutate(Mississippi1902,
                         LMopt_capital_pc=lag(Mopt_capital_pc), LMexcess_capital_pc= lag(Mexcess_capital_pc))
#>
```

Let's start again to estimate the banking effect on the total production per capita, including the new requirement of 1900.

**Task:**

Create a regression for `ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc`. Year and county fixed effects should be included. Use again `felm` for the estimation. Delete `#` and replace `??`.  
```{r}
#< task
#summary(??(ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc| ?? + countynum, data=Mississippi1902))
#>
summary(felm(ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc| year + countynum, data=Mississippi1902))
```

Including the information about 1902, the total production per person increases for about `20.77%` if opening a new national bank in Mississippi. Compare the result to the one where `50.000$` was the minimum size (12.3%). Since `20.77% > 12.3%`, the new lower minimum capital requirement of 25.000$ introduced in 1900 has a higher effect on the production than before. We can assume that a lot of national banks were constrained by the minimum size of 50.000$ and couldn't open a bank in Mississippi before the minimum stock was reduced. In the following we check the effect for manufacturing and farm production including the information from 1902. I will give you the code, just press `check`.

```{r}
#< task_notest
#Manufacture
summary(felm(ln_manuf_val_pc ~ LMopt_capital_pc + LMexcess_capital_pc| year + countynum, data=Mississippi1902))
#>
```

```{r}
#< task_notest
#Agriculture
summary(felm(ln_farmprod_val_pc ~ LMopt_capital_pc + LMexcess_capital_pc| year + countynum, data=Mississippi1902))
#>
```

The increment of manufacturing in Mississippi is with the information of 1902 around `5.86%`, without the information it was `6.2%`. The result for the improvement of farm production per person is still negative but is now higher than before `-13,3% < -7.75%`. 

How can we interpret this result? Before the minimum capital stock was lowered, we assumed that Mississippi focused on manufacturing rather than on agriculture. With the information about the 25.000$ capital stock requirement, we can see that they still promote industrialization but also focus now on farm production. The number of national banks raised, thus they had more working capital for farmers as well. Therefore, the farm production could increase. But after all, we can still assume that Mississippi was a state of manufacture. 


#### Estimation with conditional mean for the whole United States

Mississippi is a small state in the United States. Thus, we can't make a conclusion for the whole USA based in our analysis. Since Fulford did the analysis for the whole country by using the county conditional mean, we will take a look at his results. You can find the following shown results in his paper on page 934. Notice that a one-period lag and information about 1902 are included. Furthermore, year and county fixed effects are respected. Excess capital and optimal capital per capita are measured again in units of 50.000$ per average population of 14.755. Thus, they show the effect of adding a bank of minimum size to a county. Be aware that just rural counties are included in the estimation.


<style>
.mytable {
  border-collapse: collapse;
  margin-bottom: 15px;
}
.mytable td {
  padding: 10px 15px;
}
.mytable th {
  padding: 10px 15px;
  border-bottom: 1px solid #ddd;
}
.right-border {
  border-right: 1px solid #ddd;
}
.bigger-font-size {
  font-size: 18px;
}
</style>
  
  <table class="mytable">
  <tr>
  <th class="right-border"> </th>
  <th>Log Total Production Value per Capita x 100</th>
  <th>Log Manufacturing Value per Capita x 100</th>
  <th>Log Farm Production Value per Capita x 100</th>
  </tr>
  <tr>
  <th class="right-border">Excess capital per capita</th>
  <th>9.872</th>
  <th>5.828</th>
  <th>13.160</th>
  </tr>
  <tr>
  <th class="right-border">Optimal capital per capita</th>
  <th>3.451</th>
  <th>2.440</th>
  <th>1.988</th>
  </tr>
  <tr>
  </table>
  

Study the table and try to answer the following questions.

**Questions:**

#< quiz "Table1"
question: How high is the increasing of farm production per person when adding banking with minimum capital stock? Please type in a value with three digits after the point. 
answer: 13.160
#>

#< quiz "Table2"
question: How high is the impact of optimal capital stock per capita for the total production, caused by the minimum capital stock requirement? Please type in a decimal with three digits after the point.
answer: 3.451
#>


Extra banking with minimum capital stock would increase total production per person for about `9.87%`. The impact of optimal capital stock per capita lays here around `3.45%`, which is very close to our result in Exercise 3.2. Thus, the effect of gaining a minimum size bank is substantially larger than the endogenous effect that Fulford claims. Knowing this, we can also suggest that the pure entry effect of a national bank is really large. 

If we compare the effect of manufacturing per person being around `5.82%` and effect on the farm production of `13.16%`, we can assume that the increase of total production per person is the result of the high increase of farm production per person. The analysis excludes counties with major cities. In this areas manufacturing played a higher role than in rural counties. Therefore, we can assume that in such counties national banks may have promoted manufacturing more. All in all, we can claim that national banks increased production per capita compared to unbanked counties.

How can we compare these results to our regression equation

$$Y_{ct} = \gamma_{c} + \gamma_{t} + \gamma_{e} C_{ct}^{*}/P_{ct} + \gamma \mathbb{E}(C_{ct}/P_{ct}) + e_{ct}?$$

We already stated that $\gamma_{e}$ generates the endogenous effect of our model. To prevent endogeneity we found out that we can make use of the expected capital stock $$\mathbb{E}(C_{ct)$$ and will get the causal effect $\gamma$. In this exercise, I showed you how to make use of conditional mean to estimate our regression model properly. We received the unbiased estimator for $\gamma$ and can now conclude that **banking with minimum capital stock increased the total production per person in times of economical growth for about $10%$.**




## Exercise 6. Robustness Check

Robustness checks play an important role in the regression analysis. While regression analysis tries to find the relationship between a dependent variable and one or more explanatory variables, robustness checks try to figure out if the regression model is robust to changes. It will check how the conclusions change when the assumption changes.

In this exercise we will check our results for robustness. Therefore, we will restrict our sample in various ways.

### Not including 1902

One thing we must take care, is that counties who are far away from getting a bank can influence the results. The information from 1902 about banking with the lower minimum capital stock of 25.000$, gives us an overview of the distribution of optimal capital for counties with no banks. Without using the information from 1902 we see how the functional form of our estimation is responsible for the results.

In Exercise 5 we estimated the dependence between production per person and banking in Mississippi. Once we used the information from 1902 and one time we excluded them. We will now compare the results. First, load again the data set. Find the two mistakes in the following code.  

```{r}
#< task
#Data without information from 1902:

#Mississippi = read.csv("Mississippi.csv", header=TRUE, sep="\t")+
#              mutate(LMopt_capital_pc=lag(Mopt_capital_pc), LMexcess_capital_pc= lag(Mexcess_capital_pc))

#Data with information from 1902:

#Mississippi1902 = read.csv("Mississippi25.csv", header=TRUE, sep="\t")%>%
#              filter(LMopt_capital_pc=lag(Mopt_capital_pc), LMexcess_capital_pc= lag(Mexcess_capital_pc))
#>
#Data without information from 1902:
Mississippi = read.csv("Mississippi.csv", header=TRUE, sep="\t")%>%
              mutate(LMopt_capital_pc=lag(Mopt_capital_pc), LMexcess_capital_pc= lag(Mexcess_capital_pc))
#Data with information from 1902:
Mississippi1902 = read.csv("Mississippi25.csv", header=TRUE, sep="\t")%>%
              mutate(LMopt_capital_pc=lag(Mopt_capital_pc), LMexcess_capital_pc= lag(Mexcess_capital_pc))
```

#< award "Master of Data Handling"
Congratulations! You reached the highest award of dealing with data in R.
#>

**Task:**

Compare the results of the estimation `ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc`, once using information from 1902 and once not. Delete `#` and replace `??`.

```{r results='asis'}
#< task
#??(felm(ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi),           felm(ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi1902), type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE, column.labels=c("Not including 1902 ", "Including 1902 "))
#>
stargazer(felm(ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi),           felm(ln_tval_pc ~ LMopt_capital_pc + LMexcess_capital_pc|year+countynum, data=Mississippi1902), type="html", digits=3, report="vcsp*", omit.stat="ser", model.numbers=FALSE, column.labels=c("Not including 1902 ", "Including 1902 "))
```

#< award "Master of Regressions"
Awesome! Now you are a master of regressions. You know how to compile them, how to arrange the output and how to interpret the results.
#>


The estimation result with the information of 1902 is higher than without it. Not using the information of 1902, counties with no bank could be anywhere below $C^{*}$, the optimal capital. Maybe they are very near to gaining $C^{*}$ and restricted by the requirement of opening a national bank. But maybe they are far away of the optimal capital and wouldn't even open a national bank, if there hasn't been the capital stock requirement. Including the extra information from 1902, we can claim that counties without a bank are even more far away of gaining a national bank since their capital stock must be now smaller than $C^{*}_{25}$, the optimal capital with the 25.000$ minimum capital stock rule. These counties are now far away from the entry point and don't influence our result anymore. We can suggest now that the estimation is robust.

We want to check robustness also for our whole data set and for the estimation results for the whole USA. Therefore, we will look at the results of Fulford, which you can find on page 935 in his paper. In the estimation there are just `rural counties` included and `one-period lags`.Here are the results for the `mean excess capital per capita`.

<style>
.mytable {
  border-collapse: collapse;
  margin-bottom: 15px;
}
.mytable td {
  padding: 10px 15px;
}
.mytable th {
  padding: 10px 15px;
  border-bottom: 1px solid #ddd;
}
.right-border {
  border-right: 1px solid #ddd;
}
.bigger-font-size {
  font-size: 18px;
}
</style>
  
  <table class="mytable">
  <tr>
  <th class="right-border">Dependent variable</th>
  <th>Using 1902</th>
  <th>Not Using 1902</th>
  </tr>
  <tr>
  <th class="right-border">Log total production per capita x 100</th>
  <th>10.077</th>
  <th>11.084</th>
  </tr>
  <tr>
  <th class="right-border">Log manufacturing production per capita x 100</th>
  <th>5.828</th>
  <th>3.706</th>
  </tr>
  <tr>
  <th class="right-border">Log farm production per capita x 100</th>
  <th>13.162</th>
  <th>13.599</th>
  </tr>
  <tr>
  </table>
  

Study the table. Comparing the results between including 1902 and not, we see that they nearly didn't change. We can claim the same things as for the Mississippi estimation. The counties with no banks will be even more far away from the entry point including information of 1902, than not. Thus, many of them won't be included in the estimation when using 1902. The number of included counties in this case is smaller compared to not using 1902. That means, the estimation with not using 1902 has more counties included with no precise information about them. But this still won't change the results. The estimation approach is robust.


### Excluding the South

In our estimations we always used rural counties and excluded counties with major cities. But the rural counties can also be separated in union rural counties and not union rural counties. 

After the civil war was finished, the states of the CSA were again taken over by the USA. But the CSA lost a lot of people during the war and consequently became poor . Thus, the population were there on a minimum. Furthermore, they were poorly banked. Remember our analysis from Mississippi, which wasn't a union state. There we saw that the state had a lot of unbanked counties. Our estimation could be influenced by these poorly banked states in the `South`. Also, there were some counties in the `West` with a low number of banks and a small population, compared to the counties in the `North` and `Mid-West`. Thus, we want to check the results by excluding the South and the West and just take the union rural states in the sample. Have a look, at the union states on the following map. 

**Task:**

Press `check`.

```{r}
#< task_notest
require(maps)
library(maps)
Union = c("california", "connecticut", "delaware", "illinois", "indiana", "iowa", "kansas", "kentucky", "maine", "maryland", "massachusetts", "michigan", "minnesota", "missouri", "nevada", "new hampshire", "new jersey", "new york", "ohio", "oregon", "pennsylvania", "rhode island", "vermont", "west virginia", "wisconsin")
#namevec <- map(database = "state", col = "blue",fill=T, namesonly=TRUE)
map(database = "state", col = c("white", "blue")[1+(map(database = "state", col = "blue",fill=T, namesonly=TRUE) %in% tolower(Union) )],fill=T )
#>
```



We take the results of the estimation with conditional mean just using union rural counties, as Fulford displayed.

<style>
.mytable {
  border-collapse: collapse;
  margin-bottom: 15px;
}
.mytable td {
  padding: 10px 15px;
}
.mytable th {
  padding: 10px 15px;
  border-bottom: 1px solid #ddd;
}
.right-border {
  border-right: 1px solid #ddd;
}
.bigger-font-size {
  font-size: 18px;
}
</style>
  
  <table class="mytable">
  <tr>
  <th class="right-border">Dependent variable</th>
  <th>Using 1902</th>
  <th>Not Using 1902</th>
  </tr>
  <tr>
  <th class="right-border">Log total production per capita x 100</th>
  <th>8.890</th>
  <th>10.574</th>
  </tr>
  <tr>
  <th class="right-border">Log manufacturing production per capita x 100</th>
  <th>10.560</th>
  <th>10.308</th>
  </tr>
  <tr>
  <th class="right-border">Log farm production per capita x 100</th>
  <th>16.198</th>
  <th>14.415</th>
  </tr>
  <tr>
  </table>

As you can compare the upper table (10.1% using 1902 and 11.1% not using 1902), the results excluding the South didn't change nearly.   

**Question:**
#< quiz "Robustness"
question: Do you think the estimation is robust based on the result when excluding the South?
sc:
  - yes*
  - no
failure: Try again!
success: Correct! Since the results excluding the south nearly didn't change, including them won't influence our estimation. The approach is robust.

#>

Compare the result above, between including information from 1902 and not. E.g. for manufacturing production using information from 1902 leads to a higher output of `10.56%`, not using the information lead to `10.31%` output increase. Again, the results largely unchanged and the robustness we claimed before is holding for only union states as well. 



## Exercise 7. Conclusion

We are all used to online banking and getting our money where and when we want to. Though in the past it wasn't easy like today. The aim of the banks was to provide money exchange and to make commerce easier. Furthermore, they tried to connect places financially.

National banks played an important role in the payment system. They could provide loans and currency in form of national notes. Thus, the exchange of goods got comfortable and more simple. They couldn't do long-term loans but could provide working capital to farmers and producers. We found out that the presence of a national bank in a rural county increased production per capita by `10%`. Thus, we can claim that providing working capital played an important role in a growing economy rather than providing debit. Furthermore, we saw that national banks promoted both manufacturing and farm production. Since the United States during the analysed period could be compared to today developing countries, we can assume that the services of national banking played an important role in the development of the USA. Like Fulford (2015, p. 937) says, "Facilitating a payments system, providing working capital, and funding goods in transit are key functions of banks during periods of growth and development".


Finally, you can display all awards that you collected in this problem set. Just press `edit` and `check` afterwards. The maximum number of achievable awards is `12`.

```{r optional=TRUE}
#< task
awards()
#>
```



## Exercise 8. References

### Bibliography

- Auer, B., Rottmann, H. (2010): *Statistik und Oekonometrie fuer Wirtschaftswissenschaftler: Eine anwednungsorientierte Einfuehrung*. 1. Edition. Wiesbaden: Gabler Verlag.

- Brady, M.B. (1977): *Mathew Brady's Illustrated History of the Civil War with 737 Brady Photographs*. New York: Fairfax Press.

- Fahrmeir, L., Heumann, C., Kuenstler, R., Pigeot, I., Tutz, G. (2016): *Statistik: Der Weg zur Datenanalyse*. 8. Edition. Heidelberg: Springer-Verlag.

- Fahrmeir, L., Kneib, T., Lang,S. (2009): *Regression: Modelle, Methoden und Anwendungen*. 2. Edition. Heidlberg: Springer Verlag.

- Faraway, J.J. (2006): *Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models*. Boca Raton: Taylor and Francis Group.

- Field, A., Miles, J., Field, Z. (2012): *Discovering Statistics Using R*. 1. Edition. London: SAGE Publications Ltd.

- Fulford, S.L. (2015): *How Important are Banks for Development? National Banks in the United States, 1870-1900*. The Review of Economics and Statistics. 97(5):912-938.

- Hahn, J., Todd, P., Van der Klaauw, W. (2001): *Identification and Estimation of Treatment Effects with a Regression-Discontinuity Design*. Econometrica 69. The Econometric Society. 201-209.

- Hatzinger, R., Hornik, K., Nagel, H. (2011): *R Einfuehrung durch angewandte Statistik*. Muenchen: Pearson Studium

- Imbens, G.W., Lemieux, T. (2008): *Regression Discontinuity Desgins: A Guide to Practice*. Journal of Econometrics 142. 615-635.

- Kennedy, P. (2008): *A Guide to Econometrics*. 6. Edition. Malden: Blackwell Publishing.

- Kohler, U., Kreuter, F. (2012): *Datenanalyse mit Stata: Allgemeine Konzepte der Datenanalyse und ihre praktische Anwendung*. 4. Edition. Muenchen: Oldenbourg Wissenschaftsverlag GmbH.

- Luhmann, M. (2013): *R fuer Einsteiger: Einfuehrung in die Statistiksoftware fuer die Sozialwissenschaften*. 3. Edition. Basel: Beltz Verlag.

- Maindonald, J., and Braun, J.W. (2010): *Data Analysis and Graphics Using R: An Example-Based Approach*. 3. Edition. Cambridge: Cambridge University Press.

- Panik, M.J. (2012): *Statistical Inference: A Short Course*. Online Resource. Hoboken, N.J: Wiley.

- Varian, H.R. (2011): *Grundzuege der Mikrooekonomik*. 8. Edition. Muenchen: Oldenbourg Wissenschaftsverlag GmbH.

- Winker, P. (2007): *Empirische Wirtschaftsforschung und Oekonometrie*. 3. Edition. Heidelberg: Springer-Verlag.

- Wooldrige, J.M. (2016): *Introductory Econometrics: A Modern Approach*. 6. Edition. Boston, MA: Cengage Learning

- Wollschlaeger, D. (2014): *Grundlagen der Datenanalyse mit R: Eine anwendungsorientierte Einfuehrung*. 3. Edition. Berlin: Springer-Verlag.


### R Packages

- Becker, R.A., Wilks, A.R, Brownrigg, R., Minka, T.P., Deckmyn, A. (2018): *maps*. *Draw Geographical Maps*. R package verion 3.3.0. https://cran.r-project.org/web/packages/maps/index.html.

- Croissant, Y., Millo, G., Tappe, K. (2017): *plm*. *Linear Models for Panel Data*. R package versionhttps://cran.r-project.org/web/packages/plm/index.html.

- Gaure, S. (2016): *lfe*. *Linear Group Fixed Effects*. R package version 2.5-1998. https://cran.r-project.org/web/packages/lfe/lfe.pdf.

- Hlavac, M. (2015): *stargazer*. *Well-Formatted Regression and Summary Statistics Tables*. R package version 5.2. https://cran.r-project.org/web/packages/stargazer/stargazer.pdf.

- Kranz, S. (2015): *RTutor*. *R Problem Sets with Automatic Test of Solution and Hints*. R package version 2015.12.16. https://github.com/skranz/RTutor.

- R Core Team (2017): *foreign*. *Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, Weka, dBase,...*. R package version 0.8-69. https://cran.r-project.org/web/packages/foreign/foreign.pdf.

- Stephens, J., Simonov, K., Xie, Y., Dong, Z., Wickham, H., Horner, J., Reikoch, Beasley, W., O'Connor, B., Warnes, G.R. (2018): *yaml*. *Methods to Convert R Data to YAML and Back*. R package version 2.2.0. https://cran.r-project.org/web/packages/yaml/index.html.

- Wickham, H. and Chang, W. (2016): *ggplot2*. *Create Elegant Data Visualisations Using the Grammar of Graphics*. R package version 2.2.1. https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf.

- Wickham, H., Francois, R., Henry, L. and Mueller, K. (2017): *dplyr*. *A Grammar of Data Manipulation*. R package version 0.7.3. https://cran.r-project.org/web/packages/dplyr/dplyr.pdf.



### Web Pages

- http://www.r-tutor.com/r-introduction/data-frame/data-import

- https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html

- https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf

- https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html

- https://cran.r-project.org/web/packages/plm/plm.pdf

- https://de.wikipedia.org/wiki/Exogenit%C3%A4t_und_Endogenit%C3%A4t

- https://www.statisticshowto.datasciencecentral.com/reverse-causality/

- https://www.encyclopedia.com/social-sciences-and-law/law/law/united-states-civil-war#1G23437704519

- http://www.let.rug.nl/usa/essays/general/a-brief-history-of-central-banking/national-banking-acts-of-1863-and-1864.php

- https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R

- https://www.mitpressjournals.org/doi/pdf/10.1162/REST_a_00546

- https://www.mitpressjournals.org/doi/suppl/10.1162/REST_a_00546/suppl_file/REST_a_00546-esupp.pdf

- https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PQ6ILM


